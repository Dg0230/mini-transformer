//! BPE (Byte-Pair Encoding) åˆ†è¯å™¨æ¼”ç¤º
//!
//! å±•ç¤º BPE åˆ†è¯å™¨çš„å·¥ä½œåŸç†å’Œä½¿ç”¨æ–¹æ³•

use mini_transformer::{BPETokenizer, BPEConfig};

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     BPE (Byte-Pair Encoding) åˆ†è¯å™¨æ¼”ç¤º      â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 1. BPE åŸºæœ¬æ¦‚å¿µ
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("1. BPE åŸºæœ¬æ¦‚å¿µ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("ä»€ä¹ˆæ˜¯ BPE?");
    println!("  - BPE = Byte-Pair Encoding (å­—èŠ‚å¯¹ç¼–ç )");
    println!("  - ä¸€ç§æ•°æ®å‹ç¼©ç®—æ³•ï¼Œç”¨äºå­è¯åˆ†è¯");
    println!("  - é€šè¿‡è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹æ¥æ„å»ºè¯æ±‡è¡¨\n");

    println!("ä¸ºä»€ä¹ˆä½¿ç”¨ BPE?");
    println!("  âœ… è§£å†³ OOV (Out-of-Vocabulary) é—®é¢˜");
    println!("  âœ… å‡å°‘è¯æ±‡è¡¨å¤§å°ï¼ˆç›¸æ¯”å­—ç¬¦çº§ï¼‰");
    println!("  âœ… å¤„ç†ç½•è§è¯å’Œå½¢æ€å˜åŒ–");
    println!("  âœ… ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ‡å‡†åˆ†è¯æ–¹æ³•\n");

    println!("å·¥ä½œæµç¨‹:");
    println!("  1. åˆå§‹åŒ–ï¼šæ‰€æœ‰å­—ç¬¦ä½œä¸ºåˆå§‹ tokens");
    println!("  2. ç»Ÿè®¡ï¼šæ‰¾å‡ºæœ€é¢‘ç¹çš„ç›¸é‚»å­—ç¬¦å¯¹");
    println!("  3. åˆå¹¶ï¼šå°†å­—ç¬¦å¯¹åˆå¹¶ä¸ºæ–°çš„ token");
    println!("  4. é‡å¤ï¼šç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°\n");

    // ============================================================================
    // 2. è®­ç»ƒ BPE æ¨¡å‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("2. è®­ç»ƒ BPE æ¨¡å‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // å‡†å¤‡è®­ç»ƒè¯­æ–™
    let corpus: Vec<String> = vec![
        "hello world",
        "hello rust",
        "world of rust",
        "rust programming",
        "hello programming",
        "hello hello",
        "rust rust rust",
        "world world",
    ]
    .iter()
    .map(|s| s.to_string())
    .collect();

    println!("è®­ç»ƒè¯­æ–™:");
    for (i, text) in corpus.iter().enumerate() {
        println!("  {}: \"{}\"", i + 1, text);
    }
    println!();

    // åˆ›å»º BPE åˆ†è¯å™¨
    let config = BPEConfig::new(100).with_min_frequency(1);
    let mut tokenizer = BPETokenizer::new(config);

    println!("å¼€å§‹è®­ç»ƒ...\n");

    let start = std::time::Instant::now();
    tokenizer.train(&corpus);
    let duration = start.elapsed();

    println!("è®­ç»ƒå®Œæˆï¼è€—æ—¶: {:.2}s\n", duration.as_secs_f32());

    // æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    let stats = tokenizer.stats();
    println!("è¯æ±‡è¡¨ç»Ÿè®¡:");
    println!("  è¯æ±‡è¡¨å¤§å°: {}", stats.vocab_size);
    println!("  åˆå¹¶æ“ä½œæ•°: {}", stats.num_merges);
    println!("  å¹³å‡ token é•¿åº¦: {:.2}", stats.avg_token_length);
    println!("  æœ€å¤§ token é•¿åº¦: {}\n", stats.max_token_length);

    // ============================================================================
    // 3. æŸ¥çœ‹è¯æ±‡è¡¨
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("3. è¯æ±‡è¡¨ç¤ºä¾‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("ç‰¹æ®Š tokens:");
    println!("  [PAD] -> {}", tokenizer.pad_id);
    println!("  [UNK] -> {}", tokenizer.unk_id);
    println!("  [BOS] -> {}", tokenizer.bos_id);
    println!("  [EOS] -> {}\n", tokenizer.eos_id);

    println!("å­¦ä¹ åˆ°çš„å­è¯ (å‰20ä¸ªï¼Œè·³è¿‡ç‰¹æ®Š tokens):");
    let mut count = 0;
    for (i, token) in tokenizer.inverse_vocab.iter().enumerate() {
        if !token.starts_with('[') {
            println!("  {:3} -> \"{}\" (é•¿åº¦: {})", i, token, token.chars().count());
            count += 1;
            if count >= 20 {
                break;
            }
        }
    }
    println!();

    // ============================================================================
    // 4. ç¼–ç å’Œè§£ç æ¼”ç¤º
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("4. ç¼–ç å’Œè§£ç æ¼”ç¤º");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let test_texts = vec![
        "hello world",
        "rust programming",
        "hello rust programming",
        "unknown word here",
    ];

    println!("æ–‡æœ¬ç¼–ç /è§£ç ç»“æœ:");
    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚ åŸæ–‡                   â”‚ Token IDs               â”‚ è§£ç ç»“æœ               â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

    for text in test_texts {
        let token_ids = tokenizer.encode(text);
        let decoded = tokenizer.decode(&token_ids);

        let ids_str = if token_ids.len() > 10 {
            format!(
                "[{}, ..., {}] ({} tokens)",
                &token_ids[..3].iter().map(|id| id.to_string()).collect::<Vec<_>>().join(", "),
                &token_ids[token_ids.len() - 3..].iter().map(|id| id.to_string()).collect::<Vec<_>>().join(", "),
                token_ids.len()
            )
        } else {
            format!(
                "[{}] ({} tokens)",
                token_ids.iter().map(|id| id.to_string()).collect::<Vec<_>>().join(", "),
                token_ids.len()
            )
        };

        println!(
            "â”‚ {:22} â”‚ {:23} â”‚ {:22} â”‚",
            text, ids_str, decoded
        );
    }

    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    println!("ğŸ’¡ è§‚å¯Ÿ:");
    println!("   - å¸¸è§è¯ï¼ˆhello, rust, worldï¼‰è¢«æ•´ä½“ç¼–ç ");
    println!("   - ç½•è§è¯ï¼ˆunknown, hereï¼‰è¢«æ‹†åˆ†ä¸ºå­—ç¬¦");
    println!("   - ç¼–ç /è§£ç æ˜¯å¯é€†çš„ï¼ˆå®Œç¾è¿˜åŸï¼‰\n");

    // ============================================================================
    // 5. å­è¯åˆ†è§£ç¤ºä¾‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("5. å­è¯åˆ†è§£ç¤ºä¾‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let word = "programming";
    let token_ids = tokenizer.encode(word);

    println!("å•è¯: \"{}\"", word);
    println!("Token IDs: {:?}", token_ids);
    print!("å­è¯åˆ†è§£: ");

    for (i, &id) in token_ids.iter().enumerate() {
        if id == tokenizer.pad_id {
            continue;
        }
        let token = tokenizer.id_to_token(id);
        if i > 0 && token_ids[i - 1] != tokenizer.pad_id {
            print!(" + ");
        }
        print!("\"{}\"", token);
    }
    println!("\n");

    // ============================================================================
    // 6. å¯¹æ¯”ä¸åŒè¯æ±‡è¡¨å¤§å°
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("6. ä¸åŒè¯æ±‡è¡¨å¤§å°çš„å½±å“");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let vocab_sizes = vec![20, 50, 100];
    let test_text = "hello rust programming world";

    println!("æµ‹è¯•æ–‡æœ¬: \"{}\"\n", test_text);

    for vocab_size in vocab_sizes {
        let mut tokenizer_small = BPETokenizer::new(BPEConfig::new(vocab_size).with_min_frequency(1));
        tokenizer_small.train(&corpus);

        let token_ids = tokenizer_small.encode(test_text);
        let subwords: Vec<String> = token_ids
            .iter()
            .filter(|&&id| id != tokenizer_small.pad_id)
            .map(|&id| tokenizer_small.id_to_token(id))
            .collect();

        println!("è¯æ±‡è¡¨å¤§å°: {} -> {} tokens", vocab_size, token_ids.len());
        println!("  åˆ†è§£: {}", subwords.join(" + "));
    }
    println!();

    // ============================================================================
    // 7. ä¸å­—ç¬¦çº§å’Œè¯çº§åˆ†è¯å¯¹æ¯”
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("7. ä¸å…¶ä»–åˆ†è¯æ–¹æ³•å¯¹æ¯”");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚ åˆ†è¯çº§åˆ«        â”‚ è¯æ±‡è¡¨å¤§å°    â”‚ OOV å¤„ç†     â”‚ ç¤ºä¾‹         â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("â”‚ å­—ç¬¦çº§          â”‚ å¾ˆå° (~100)  â”‚ æ—  OOV       â”‚ h-e-l-l-o    â”‚");
    println!("â”‚ è¯çº§            â”‚ å¾ˆå¤§ (>100K) â”‚ ä¸¥é‡ OOV     â”‚ hello        â”‚");
    println!("â”‚ å­è¯çº§ (BPE)    â”‚ ä¸­ç­‰ (~10K)  â”‚ è½»å¾® OOV     â”‚ hell-o       â”‚");
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    // ============================================================================
    // 8. å®é™…åº”ç”¨ç¤ºä¾‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("8. å®é™…åº”ç”¨ç¤ºä¾‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("1. ä¿å­˜å’ŒåŠ è½½è¯æ±‡è¡¨:");
    println!("```rust");
    println!("// ä¿å­˜è¯æ±‡è¡¨");
    println!("tokenizer.save_vocab(\"vocab.txt\")?;");
    println!();
    println!("// åŠ è½½è¯æ±‡è¡¨");
    println!("let mut new_tokenizer = BPETokenizer::new(config);");
    println!("new_tokenizer.load_vocab(\"vocab.txt\")?;");
    println!("```\n");

    println!("2. é¢„å¤„ç†æ–‡æœ¬:");
    println!("```rust");
    println!("let text = \"Hello, World!\";");
    println!("let tokens = tokenizer.encode(text);");
    println!("let decoded = tokenizer.decode(&tokens);");
    println!("assert_eq!(text, decoded);");
    println!("```\n");

    println!("3. å¤„ç†æœªçŸ¥è¯:");
    println!("```rust");
    println!("let unknown_text = \"ä¸€äº›æœªçŸ¥è¯\";");
    println!("let tokens = tokenizer.encode(unknown_text);");
    println!("// BPE ä¼šå°†æœªçŸ¥è¯æ‹†åˆ†ä¸ºå·²çŸ¥çš„å­è¯æˆ–å­—ç¬¦");
    println!("// ä¸ä¼šäº§ç”Ÿ UNK tokenï¼ˆé™¤éå­—ç¬¦ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼‰");
    println!("```\n");

    // ============================================================================
    // 9. æŠ€æœ¯ç»†èŠ‚
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("9. æŠ€æœ¯ç»†èŠ‚");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("1. BPE ç®—æ³•å¤æ‚åº¦:");
    println!("   - è®­ç»ƒ: O(N Ã— M Ã— V)");
    println!("     N = è¯­æ–™å¤§å°, M = å¹³å‡è¯é•¿, V = ç›®æ ‡è¯æ±‡è¡¨å¤§å°");
    println!("   - ç¼–ç : O(L Ã— M)");
    println!("     L = åˆå¹¶è§„åˆ™æ•°, M = è¯é•¿");
    println!("   - è§£ç : O(T)");
    println!("     T = token æ•°é‡\n");

    println!("2. æ¨èé…ç½®:");
    println!("   - è¯æ±‡è¡¨å¤§å°: 10K - 100K");
    println!("   - æœ€å°é¢‘ç‡: 2 - 10");
    println!("   - ç‰¹æ®Š tokens: [PAD], [UNK], [BOS], [EOS]\n");

    println!("3. æ”¹è¿›ç‰ˆæœ¬:");
    println!("   - Byte-level BPE (GPT-2/3): å¤„ç†æ‰€æœ‰ Unicode å­—ç¬¦");
    println!("   - WordPiece (BERT): ä¼˜åŒ–ä¼¼ç„¶è€Œéé¢‘ç‡");
    println!("   - Unigram: åŸºäºè¯­è¨€æ¨¡å‹çš„å­è¯é€‰æ‹©\n");

    println!("4. å®ç°ç¤ºä¾‹:");
    println!("```rust");
    println!("use mini_transformer::{{BPETokenizer, BPEConfig}};");
    println!();
    println!("// åˆ›å»ºé…ç½®");
    println!("let config = BPEConfig::new(10000).with_min_frequency(2);");
    println!();
    println!("// è®­ç»ƒåˆ†è¯å™¨");
    println!("let mut tokenizer = BPETokenizer::new(config);");
    println!("tokenizer.train(&corpus);");
    println!();
    println!("// ç¼–ç æ–‡æœ¬");
    println!("let tokens = tokenizer.encode(\"hello world\");");
    println!();
    println!("// è§£ç  tokens");
    println!("let text = tokenizer.decode(&tokens);");
    println!("```");

    println!();
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     æ¼”ç¤ºå®Œæˆï¼                               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    println!("ğŸ’¡ å®è·µå»ºè®®:");
    println!("   - BPE æ˜¯ç°ä»£ LLM çš„æ ‡å‡†åˆ†è¯æ–¹æ³•");
    println!("   - æ¨èè¯æ±‡è¡¨å¤§å°: 10K - 50K");
    println!("   - åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè®­ç»ƒä»¥è·å¾—æœ€ä½³æ•ˆæœ");
    println!("   - ä¿å­˜è®­ç»ƒå¥½çš„è¯æ±‡è¡¨ä»¥ä¾¿å¤ç”¨");
    println!("   - è€ƒè™‘ä½¿ç”¨ SentencePiece æˆ– tokenizers åº“ç”¨äºç”Ÿäº§ç¯å¢ƒ");
}
