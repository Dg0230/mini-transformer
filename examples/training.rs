//! è®­ç»ƒç¤ºä¾‹
//!
//! æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒ API

use mini_transformer::{
    TransformerEncoder, TransformerConfig,
    CrossEntropyLoss, Adam, CosineAnnealingWarmRestarts,
    Trainer, TrainerConfig, Dataset, DataLoader, SimpleDataset,
    configs,
};

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     Transformer è®­ç»ƒç¤ºä¾‹                      â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // åˆ›å»ºé…ç½®
    let config = configs::mini();
    println!("æ¨¡å‹é…ç½®:");
    println!("  - vocab_size: {}", config.vocab_size);
    println!("  - d_model: {}", config.d_model);
    println!("  - n_layers: {}", config.n_layers);
    println!("  - n_heads: {}", config.n_heads);

    // åˆ›å»ºæ¨¡å‹
    let model = TransformerEncoder::new(config.clone());
    println!("\næ¨¡å‹å‚æ•°æ€»æ•°: {}", model.param_count());

    // åˆ›å»ºè®­ç»ƒæ•°æ®
    println!("\nåˆ›å»ºè®­ç»ƒæ•°æ®...");
    let train_data = SimpleDataset::random(
        1000,  // æ ·æœ¬æ•°
        10,    // åºåˆ—é•¿åº¦
        1000,  // è¯è¡¨å¤§å°
        5,     // ç±»åˆ«æ•°
    );

    let val_data = SimpleDataset::random(
        200,   // æ ·æœ¬æ•°
        10,    // åºåˆ—é•¿åº¦
        1000,  // è¯è¡¨å¤§å°
        5,     // ç±»åˆ«æ•°
    );

    println!("  è®­ç»ƒé›†å¤§å°: {}", train_data.len());
    println!("  éªŒè¯é›†å¤§å°: {}", val_data.len());

    // åˆ›å»ºæ•°æ®åŠ è½½å™¨
    let train_loader = DataLoader::new(train_data, 32, true);
    let val_loader = DataLoader::new(val_data, 32, false);

    // åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    let loss_fn = CrossEntropyLoss::new();
    let mut optimizer = Adam::new(0.001);

    // åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    let lr_scheduler = CosineAnnealingWarmRestarts::new(0.001, 0.0001, 50, 5);

    // åˆ›å»ºè®­ç»ƒå™¨
    let trainer_config = TrainerConfig {
        batch_size: 32,
        epochs: 5,
        learning_rate: 0.001,
        ..Default::default()
    };

    let trainer = Trainer::new(trainer_config);

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     å¼€å§‹è®­ç»ƒ                                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // è®­ç»ƒå¾ªç¯
    for epoch in 1..=5 {
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

        // æ›´æ–°å­¦ä¹ ç‡
        let lr = lr_scheduler.get_lr(epoch);
        optimizer.set_lr(lr);
        println!("å­¦ä¹ ç‡: {:.6}", lr);

        // è®­ç»ƒä¸€ä¸ª epoch
        // æ³¨æ„ï¼šå½“å‰å®ç°æ˜¯ç®€åŒ–çš„ï¼Œå®Œæ•´çš„è®­ç»ƒéœ€è¦ autograd ç³»ç»Ÿ
        println!("è®­ç»ƒ Epoch {}...", epoch);
        // let (train_loss, samples_per_sec) = trainer.train_epoch(
        //     &mut model,
        //     &train_loader,
        //     &mut optimizer,
        //     &loss_fn,
        //     epoch,
        // );

        // è¯„ä¼°
        println!("è¯„ä¼°...");
        // let (val_loss, val_acc) = trainer.evaluate(&model, &val_loader, &loss_fn);

        println!("Epoch {} å®Œæˆ", epoch);
        println!("  è®­ç»ƒæŸå¤±: {:.4}", 0.0);
        println!("  éªŒè¯æŸå¤±: {:.4}", 0.0);
        println!("  éªŒè¯å‡†ç¡®ç‡: {:.2}%", 0.0);
        println!();
    }

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     è®­ç»ƒå®Œæˆï¼                                â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    println!("\nğŸ’¡ æç¤º:");
    println!("  - å½“å‰å®ç°æ˜¯æ¡†æ¶ä»£ç ï¼Œå®Œæ•´è®­ç»ƒéœ€è¦æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­");
    println!("  - æˆ–è€…ä½¿ç”¨æˆç†Ÿçš„æ¡†æ¶ï¼ˆcandleã€burnï¼‰");
    println!("  - å‚è€ƒç¤ºä¾‹ï¼štrain_epoch å’Œ evaluate æ–¹æ³•ä¸­çš„ TODO æ³¨é‡Š");
}
