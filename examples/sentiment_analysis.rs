//! æƒ…æ„Ÿåˆ†æè®­ç»ƒç¤ºä¾‹
//!
//! ä½¿ç”¨çœŸå®çš„æ–‡æœ¬æ•°æ®è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»è®­ç»ƒ

use mini_transformer::{
    TrainableTransformer, TextClassificationDataset, Vocabulary,
    CheckpointManager, TrainingHistory, TensorExt,
};
use ndarray::Array2;
use std::time::Instant;

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     Transformer æƒ…æ„Ÿåˆ†æè®­ç»ƒ                 â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // åˆ›å»ºæ•°æ®é›†
    println!("ğŸ“Š åŠ è½½æƒ…æ„Ÿåˆ†ææ•°æ®é›†...");
    let dataset = mini_transformer::create_sentiment_dataset();

    println!("  è¯æ±‡è¡¨å¤§å°: {}", dataset.vocab.vocab_size());
    println!("  è®­ç»ƒæ ·æœ¬: {}", dataset.train_len());
    println!("  æµ‹è¯•æ ·æœ¬: {}", dataset.test_len());
    println!("  ç±»åˆ«æ•°: {} (0=æ¶ˆæ, 1=ç§¯æ)\n", dataset.n_classes);

    // å‡†å¤‡æ•°æ®
    let max_seq_len = 10; // æœ€å¤§åºåˆ—é•¿åº¦
    let (train_inputs, train_targets) = dataset.encode_train(max_seq_len);
    let (test_inputs, test_targets) = dataset.encode_test(max_seq_len);

    // å°† Array2 è½¬æ¢ä¸º Vec<Vec<usize>>
    let train_inputs_vec: Vec<Vec<usize>> = train_inputs
        .rows()
        .into_iter()
        .map(|row| row.to_vec())
        .collect();

    let test_inputs_vec: Vec<Vec<usize>> = test_inputs
        .rows()
        .into_iter()
        .map(|row| row.to_vec())
        .collect();

    // åˆ›å»ºæ¨¡å‹
    println!("ğŸ—ï¸  æ„å»ºæ¨¡å‹...");
    let mut model = TrainableTransformer::new(
        dataset.vocab.vocab_size(),  // vocab_size
        128,                          // d_model
        4,                            // n_heads
        2,                            // n_layers
        256,                          // d_ff
        max_seq_len,                  // max_seq_len
        dataset.n_classes,            // n_classes
    );

    println!("  æ¨¡å‹å‚æ•°æ€»æ•°: {}\n", model.param_count());

    // è®­ç»ƒé…ç½®
    let epochs = 50;
    let learning_rate = 0.01;
    let patience = 10;

    // è®­ç»ƒå‰è¯„ä¼°
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("è®­ç»ƒå‰è¯„ä¼°:");
    let (init_train_loss, init_train_acc) = model.evaluate(&train_inputs_vec, &train_targets);
    let (init_test_loss, init_test_acc) = model.evaluate(&test_inputs_vec, &test_targets);
    println!("  è®­ç»ƒé›†: Loss={:.4}, Acc={:.2}%", init_train_loss, init_train_acc * 100.0);
    println!("  æµ‹è¯•é›†: Loss={:.4}, Acc={:.2}%\n", init_test_loss, init_test_acc * 100.0);

    // è®­ç»ƒæ¨¡å‹
    let start_time = Instant::now();
    let (best_epoch, best_val_acc) = model.train(
        &train_inputs_vec,
        &train_targets,
        &test_inputs_vec,
        &test_targets,
        epochs,
        learning_rate,
        patience,
    );
    let total_time = start_time.elapsed();

    println!("\nâ±ï¸  æ€»è®­ç»ƒæ—¶é—´: {:.2} ç§’", total_time.as_secs_f32());
    println!("   å¹³å‡æ¯ä¸ª epoch: {:.2} ç§’", total_time.as_secs_f32() / best_epoch as f32);

    // æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°:");
    let (final_test_loss, final_test_acc) = model.evaluate(&test_inputs_vec, &test_targets);
    println!("  æµ‹è¯•é›† Loss: {:.4}", final_test_loss);
    println!("  æµ‹è¯•é›†å‡†ç¡®ç‡: {:.2}%", final_test_acc * 100.0);

    // æ¨ç†ç¤ºä¾‹
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æ¨ç†ç¤ºä¾‹:");

    let test_sentences = vec![
        "this movie is great and wonderful",
        "this movie is terrible and boring",
        "I love this film it is amazing",
        "I hate this film it is awful",
    ];

    model.set_training(false);

    for sentence in test_sentences {
        let tokens = dataset.vocab.encode(sentence, max_seq_len);
        let input_batch = Array2::from_shape_vec((1, tokens.len()), tokens).unwrap();

        let logits = model.forward(&input_batch);
        let probs = logits.softmax(1);
        let prob_slice = probs.row(0);

        let pred_class = prob_slice
            .iter()
            .enumerate()
            .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
            .map(|(i, _)| i)
            .unwrap();

        let sentiment = if pred_class == 1 { "ç§¯æ" } else { "æ¶ˆæ" };
        let confidence = prob_slice[pred_class];

        println!("  æ–‡æœ¬: \"{}\"", sentence);
        println!("  é¢„æµ‹: {} (ç½®ä¿¡åº¦: {:.2}%)\n", sentiment, confidence * 100.0);
    }

    // æ¨¡å‹åˆ†æ
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("æ¨¡å‹åˆ†æ:");
    println!("  è®­ç»ƒæ ·æœ¬æ•°: {}", train_targets.len());
    println!("  æµ‹è¯•æ ·æœ¬æ•°: {}", test_targets.len());
    println!("  æ¨¡å‹å‚æ•°é‡: {}", model.param_count());
    println!("  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {:.2}%", best_val_acc * 100.0);
    println!("  æœ€ä½³ epoch: {}", best_epoch);

    // ä¸éšæœºçŒœæµ‹å¯¹æ¯”
    println!("\n  ä¸éšæœºçŒœæµ‹å¯¹æ¯”:");
    println!("    éšæœºçŒœæµ‹å‡†ç¡®ç‡: ~50.0% (äºŒåˆ†ç±»)");
    if final_test_acc > 0.5 {
        println!("    æ¨¡å‹æå‡: +{:.2}%", (final_test_acc - 0.5) * 100.0);
        println!("    âœ… æ¨¡å‹å­¦åˆ°äº†æœ‰ç”¨çš„æ¨¡å¼ï¼");
    } else {
        println!("    âš ï¸  æ¨¡å‹æ€§èƒ½æ¥è¿‘éšæœºï¼Œéœ€è¦æ›´å¤šè®­ç»ƒæˆ–æ›´å¤§æ¨¡å‹");
    }

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     è®­ç»ƒå®Œæˆï¼                                â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
}
