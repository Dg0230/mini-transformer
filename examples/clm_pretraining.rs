//! Causal Language Modeling (CLM) é¢„è®­ç»ƒç¤ºä¾‹
//!
//! æ¼”ç¤º GPT é£æ ¼çš„é¢„è®­ç»ƒï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token

use mini_transformer::{Seq2SeqTransformer, CLMConfig, create_clm_targets, Vocabulary};

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   CLM é¢„è®­ç»ƒç¤ºä¾‹ (GPT é£æ ¼)                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 1. å‡†å¤‡é¢„è®­ç»ƒæ•°æ®
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("1. å‡†å¤‡é¢„è®­ç»ƒæ•°æ®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // æ¨¡æ‹Ÿé¢„è®­ç»ƒæ–‡æœ¬
    let corpus = vec![
        "the cat sat on the mat",
        "dogs are loyal animals",
        "birds can fly high",
        "fish swim in water",
        "trees grow tall",
        "the sun is bright",
        "moon shines at night",
        "stars twinkle brightly",
        "children love to play",
        "music brings joy",
        "reading books is educational",
        "writing helps express thoughts",
        "learning new skills",
        "practice makes perfect",
        "teamwork achieves goals",
    ];

    // æ„å»ºè¯æ±‡è¡¨
    let mut vocab = Vocabulary::new();
    for text in &corpus {
        for token in text.split_whitespace() {
            if !vocab.token_to_id.contains_key(&token.to_string()) {
                let id = vocab.vocab_size;
                vocab.token_to_id.insert(token.to_string(), id);
                vocab.id_to_token.push(token.to_string());
                vocab.vocab_size += 1;
            }
        }
    }

    // æ·»åŠ ç‰¹æ®Š tokens
    let start_token_id = vocab.vocab_size;
    vocab.token_to_id.insert("<START>".to_string(), start_token_id);
    vocab.id_to_token.push("<START>".to_string());
    vocab.vocab_size += 1;

    let end_token_id = vocab.vocab_size;
    vocab.token_to_id.insert("<END>".to_string(), end_token_id);
    vocab.id_to_token.push("<END>".to_string());
    vocab.vocab_size += 1;

    println!("è¯æ±‡è¡¨å¤§å°: {}", vocab.vocab_size);
    println!("ç‰¹æ®Š tokens:");
    println!("  <UNK>=0, <PAD>=1");
    println!("  <START>={}", start_token_id);
    println!("  <END>={}\n", end_token_id);

    // ç¼–ç è¯­æ–™
    let mut encoded_corpus = Vec::new();
    for text in &corpus {
        let tokens: Vec<usize> = text.split_whitespace()
            .map(|token| *vocab.token_to_id.get(token).unwrap_or(&0))
            .collect();
        encoded_corpus.push(tokens);
    }

    println!("ç¼–ç åçš„è¯­æ–™ç¤ºä¾‹:");
    println!("  åŸå§‹: {}", corpus[0]);
    println!("  ç¼–ç : {:?}\n", encoded_corpus[0]);

    // ============================================================================
    // 2. æ¼”ç¤º CLM ç›®æ ‡åˆ›å»º
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("2. CLM ç›®æ ‡åˆ›å»ºæ¼”ç¤º");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let clm_config = CLMConfig {
        start_token_id,
        end_token_id,
        pad_token_id: 1,
    };

    println!("GPT é£æ ¼å› æœè¯­è¨€å»ºæ¨¡:\n");
    println!("  ç»™å®šå½“å‰ token åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª token");
    println!("  è¾“å…¥: [<START>, t1, t2, t3, ...]");
    println!("  ç›®æ ‡: [t1, t2, t3, ..., <END>]\n");

    for (i, tokens) in encoded_corpus.iter().take(3).enumerate() {
        let (inputs, targets) = create_clm_targets(tokens, &clm_config);

        println!("æ ·æœ¬ {}:", i + 1);
        println!("  åŸå§‹:  {}", corpus[i]);
        println!("  è¾“å…¥:  {:?}", inputs);
        println!("  ç›®æ ‡:  {:?}", targets);

        let input_text: Vec<String> = inputs.iter()
            .filter_map(|&id| vocab.id_to_token.get(id).cloned())
            .collect();
        let target_text: Vec<String> = targets.iter()
            .filter_map(|&id| vocab.id_to_token.get(id).cloned())
            .collect();

        println!("  è¾“å…¥æ–‡æœ¬: {}", input_text.join(" "));
        println!("  ç›®æ ‡æ–‡æœ¬: {}\n", target_text.join(" "));
    }

    // ============================================================================
    // 3. åˆ›å»º CLM é¢„è®­ç»ƒæ¨¡å‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("3. åˆ›å»º CLM é¢„è®­ç»ƒæ¨¡å‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut model = Seq2SeqTransformer::new(
        vocab.vocab_size,  // vocab_size
        128,               // d_model
        4,                 // n_heads
        2,                 // n_layers
        256,               // d_ff
        20,                // max_seq_len
    );

    println!("æ¨¡å‹æ¶æ„:");
    println!("  è¯æ±‡è¡¨å¤§å°: {}", vocab.vocab_size);
    println!("  æ¨¡å‹ç»´åº¦: {}", 128);
    println!("  æ³¨æ„åŠ›å¤´æ•°: {}", 4);
    println!("  Decoder å±‚æ•°: {}", 2);
    println!("  FFN ç»´åº¦: {}", 256);
    println!("  å‚æ•°æ€»æ•°: {}\n", model.param_count());

    // ============================================================================
    // 4. å‡†å¤‡è®­ç»ƒæ•°æ®
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("4. å‡†å¤‡ CLM è®­ç»ƒæ•°æ®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut train_inputs = Vec::new();
    let mut train_targets = Vec::new();
    let mut val_inputs = Vec::new();
    let mut val_targets = Vec::new();

    let n_train = (encoded_corpus.len() * 8 / 10) as usize;

    for (i, tokens) in encoded_corpus.iter().enumerate() {
        let (inputs, targets) = create_clm_targets(tokens, &clm_config);

        if i < n_train {
            train_inputs.push(inputs);
            train_targets.push(targets);
        } else {
            val_inputs.push(inputs);
            val_targets.push(targets);
        }
    }

    println!("è®­ç»ƒæ ·æœ¬: {}", train_inputs.len());
    println!("éªŒè¯æ ·æœ¬: {}\n", val_inputs.len());

    // ============================================================================
    // 5. å¼€å§‹ CLM é¢„è®­ç»ƒ
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("5. å¼€å§‹ CLM é¢„è®­ç»ƒ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut best_val_acc = 0.0;
    let mut patience_counter = 0;
    let epochs = 50;
    let learning_rate = 0.001;
    let patience = 10;

    println!("è®­ç»ƒé…ç½®:");
    println!("  Epochs: {}", epochs);
    println!("  å­¦ä¹ ç‡: {}", learning_rate);
    println!("  æ—©åœ patience: {}\n", patience);

    for epoch in 1..=epochs {
        let epoch_start = std::time::Instant::now();

        // è®­ç»ƒ
        let mut total_loss = 0.0;
        let mut total_acc = 0.0;

        for (input, target) in train_inputs.iter().zip(train_targets.iter()) {
            let (loss, acc) = model.train_step(input, target, learning_rate);
            total_loss += loss;
            total_acc += acc;
        }

        let avg_train_loss = total_loss / train_inputs.len() as f32;
        let avg_train_acc = total_acc / train_inputs.len() as f32;

        // éªŒè¯
        let mut val_loss = 0.0;
        let mut val_acc = 0.0;

        for (input, target) in val_inputs.iter().zip(val_targets.iter()) {
            model.set_training(false);
            let encoder_output = model.encode(input);

            let target_embedded = model.embedding.forward(target);
            let target_encoded = model.pos_encoding.forward(&target_embedded);
            let decoder_input = target_encoded.slice(ndarray::s![..1, ..]).to_owned();

            let logits = model.decode_step(&decoder_input, &encoder_output);

            let loss = model.compute_loss(&logits, target[target.len() - 1]);
            let accuracy = if logits.row(0).iter().enumerate()
                .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
                .map(|(i, _)| i)
                .unwrap() == target[target.len() - 1]
            {
                1.0
            } else {
                0.0
            };

            val_loss += loss;
            val_acc += accuracy;
        }

        let avg_val_loss = val_loss / val_inputs.len() as f32;
        let avg_val_acc = val_acc / val_inputs.len() as f32;

        let epoch_time = epoch_start.elapsed().as_secs_f32();

        println!(
            "Epoch {:2}/{} | Train Loss: {:.4} | Train Acc: {:.2}% | Val Loss: {:.4} | Val Acc: {:.2}% | {:.2}s",
            epoch, epochs, avg_train_loss, avg_train_acc * 100.0, avg_val_loss, avg_val_acc * 100.0, epoch_time
        );

        if avg_val_acc > best_val_acc {
            best_val_acc = avg_val_acc;
            patience_counter = 0;
            println!("  âœ¨ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {:.2}%", avg_val_acc * 100.0);
        } else {
            patience_counter += 1;
            println!("  â³ éªŒè¯å‡†ç¡®ç‡æœªæå‡ ({}/{})", patience_counter, patience);
        }

        if patience_counter >= patience {
            println!("\nâš ï¸  æ—©åœè§¦å‘ï¼éªŒè¯å‡†ç¡®ç‡å·² {} ä¸ª epoch æœªæå‡", patience);
            break;
        }

        println!();
    }

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     CLM é¢„è®­ç»ƒå®Œæˆï¼                           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•");
    println!("æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {:.2}%\n", best_val_acc * 100.0);

    // ============================================================================
    // 6. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("6. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    model.set_training(false);

    let test_prompts = vec![
        vec![start_token_id, *vocab.token_to_id.get("the").unwrap()],
        vec![start_token_id, *vocab.token_to_id.get("birds").unwrap()],
        vec![start_token_id, *vocab.token_to_id.get("dogs").unwrap()],
    ];

    for (i, prompt) in test_prompts.iter().enumerate() {
        println!("æµ‹è¯• {}:", i + 1);

        let prompt_text: Vec<String> = prompt.iter()
            .filter_map(|&id| vocab.id_to_token.get(id).cloned())
            .collect();
        println!("  æç¤º: {}", prompt_text.join(" "));

        // è´ªå©ªè§£ç ç”Ÿæˆ
        let output = model.generate_greedy(prompt, 10, start_token_id);

        let output_text: Vec<String> = output.iter()
            .filter_map(|&id| {
                if id == end_token_id {
                    Some("<END>".to_string())
                } else if id == start_token_id {
                    None
                } else {
                    vocab.id_to_token.get(id).cloned()
                }
            })
            .collect();

        println!("  ç”Ÿæˆ: {}\n", output_text.join(" "));
    }

    // ============================================================================
    // 7. æŠ€æœ¯ç»†èŠ‚
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("CLM é¢„è®­ç»ƒæŠ€æœ¯ç»†èŠ‚");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("1. å› æœè¯­è¨€å»ºæ¨¡:");
    println!("   - è‡ªå›å½’ç”Ÿæˆ: P(t_n | t_1, ..., t_{{n-1}})");
    println!("   - ä½¿ç”¨å› æœæ©ç é˜²æ­¢ä½ç½®å…³æ³¨åç»­ä½ç½®");
    println!("   - æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ª token");
    println!();

    println!("2. æŸå¤±å‡½æ•°:");
    println!("   - åœ¨æ‰€æœ‰ä½ç½®è®¡ç®—æŸå¤±");
    println!("   - ä½¿ç”¨ Cross-Entropy Loss");
    println!("   - ç´¯åŠ æ‰€æœ‰ä½ç½®çš„é¢„æµ‹æŸå¤±");
    println!();

    println!("3. æ¨¡å‹æ¶æ„:");
    println!("   - åŸºäº Transformer Decoder");
    println!("   - å•å‘ï¼ˆå› æœï¼‰æ³¨æ„åŠ›æœºåˆ¶");
    println!("   - è¾“å‡ºæŠ•å½±åˆ°è¯æ±‡è¡¨");
    println!();

    println!("4. é¢„è®­ç»ƒä¼˜åŠ¿:");
    println!("   - å­¦ä¹ ç”Ÿæˆèƒ½åŠ›");
    println!("   - é€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡");
    println!("   - ä¸ºå¯¹è¯ã€æ‘˜è¦ç­‰ä»»åŠ¡æä¾›åŸºç¡€");
    println!();

    println!("5. MLM vs CLM:");
    println!("   - MLM (BERT): åŒå‘ç†è§£ï¼Œé€‚åˆåˆ†ç±»");
    println!("   - CLM (GPT): å•å‘ç”Ÿæˆï¼Œé€‚åˆåˆ›ä½œ");
    println!();

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     CLM é¢„è®­ç»ƒç¤ºä¾‹å®Œæˆï¼                       â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("ğŸ’¡ æç¤º:");
    println!("   - å½“å‰ä½¿ç”¨å°è§„æ¨¡æ•°æ®æ¼”ç¤º");
    println!("   - å®é™…é¢„è®­ç»ƒéœ€è¦æ•°ç™¾Båˆ°æ•°T tokens");
    println!("   - GPT-3 ä½¿ç”¨ 300B tokens, GPT-4 ä½¿ç”¨æ›´å¤š");
}
