//! å®Œæ•´è®­ç»ƒç¤ºä¾‹
//!
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ—©åœå’Œ Warmup Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦è¿›è¡Œæ¨¡å‹è®­ç»ƒ

use mini_transformer::{
    TrainableTransformer, TransformerConfig, Adam, CrossEntropyLoss,
    WarmupCosineAnnealing, EarlyStopping, EarlyStoppingConfig, EarlyStoppingMode,
    SimpleDataset, DataLoader,
    LossFunction, Optimizer, LRScheduler,
};
use ndarray::Array2;
use rand::Rng;
use std::time::Instant;

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     å®Œæ•´è®­ç»ƒç¤ºä¾‹ (æ—©åœ + å­¦ä¹ ç‡è°ƒåº¦)         â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 1. è®­ç»ƒé…ç½®
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("1. è®­ç»ƒé…ç½®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // æ¨¡å‹é…ç½®
    let model_config = TransformerConfig {
        vocab_size: 1000,
        d_model: 256,
        n_heads: 8,
        n_layers: 4,
        d_ff: 1024,
        max_seq_len: 128,
        dropout: 0.1,
    };

    // è®­ç»ƒè¶…å‚æ•°
    let batch_size = 32;
    let epochs = 1;
    let learning_rate = 0.0001;  // 1e-4
    let total_steps = 5000;      // ä¼°è®¡çš„æ€»è®­ç»ƒæ­¥æ•°
    let warmup_steps = 500;      // warmup æ­¥æ•°
    let n_classes = 10;          // åˆ†ç±»æ•°

    println!("æ¨¡å‹é…ç½®:");
    println!("  è¯æ±‡è¡¨å¤§å°: {}", model_config.vocab_size);
    println!("  æ¨¡å‹ç»´åº¦: {}", model_config.d_model);
    println!("  æ³¨æ„åŠ›å¤´æ•°: {}", model_config.n_heads);
    println!("  å±‚æ•°: {}", model_config.n_layers);
    println!("  å‰é¦ˆç»´åº¦: {}\n", model_config.d_ff);

    println!("è®­ç»ƒè¶…å‚æ•°:");
    println!("  æ‰¹å¤§å°: {}", batch_size);
    println!("  æœ€å¤§è½®æ•°: {}", epochs);
    println!("  å­¦ä¹ ç‡: {:.6}", learning_rate);
    println!("  æ€»è®­ç»ƒæ­¥æ•°: {}", total_steps);
    println!("  Warmup æ­¥æ•°: {}\n", warmup_steps);

    // ============================================================================
    // 2. åˆ›å»ºæ¨¡å‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("2. åˆ›å»ºæ¨¡å‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut model = TrainableTransformer::new(
        model_config.vocab_size,
        model_config.d_model,
        model_config.n_heads,
        model_config.n_layers,
        model_config.d_ff,
        model_config.max_seq_len,
        n_classes,
    );
    println!("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼");
    println!("  å‚æ•°é‡: {:.1}M\n", model.param_count() as f32 / 1e6);

    // ============================================================================
    // 3. åˆ›å»ºæ•°æ®é›†
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("3. åˆ›å»ºæ•°æ®é›†");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let train_size = 5000;
    let val_size = 1000;
    let seq_len = 64;
    let vocab_size = model_config.vocab_size;

    println!("åˆ›å»ºè®­ç»ƒé›†: {} æ ·æœ¬", train_size);
    let train_dataset = SimpleDataset::random(train_size, seq_len, vocab_size, n_classes);

    println!("åˆ›å»ºéªŒè¯é›†: {} æ ·æœ¬\n", val_size);
    let val_dataset = SimpleDataset::random(val_size, seq_len, vocab_size, n_classes);

    let train_loader = DataLoader::new(train_dataset, batch_size, true);
    let val_loader = DataLoader::new(val_dataset, batch_size, false);

    // ============================================================================
    // 4. åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("4. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut optimizer = Adam::new(learning_rate);

    // Warmup Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦
    let lr_scheduler = WarmupCosineAnnealing::new(
        learning_rate,
        0.0,            // min_lr
        total_steps,
        warmup_steps,
    );

    println!("å­¦ä¹ ç‡è°ƒåº¦: Warmup Cosine Annealing");
    println!("  åˆå§‹å­¦ä¹ ç‡: {:.6}", lr_scheduler.get_lr(0));
    println!("  Warmup åå­¦ä¹ ç‡: {:.6}", lr_scheduler.get_lr(warmup_steps));
    println!("  è®­ç»ƒä¸­æœŸå­¦ä¹ ç‡: {:.6}", lr_scheduler.get_lr(total_steps / 2));
    println!("  æœ€ç»ˆå­¦ä¹ ç‡: {:.6}\n", lr_scheduler.get_lr(total_steps));

    // ============================================================================
    // 5. åˆ›å»ºæ—©åœå™¨
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("5. æ—©åœé…ç½®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let early_stopping_config = EarlyStoppingConfig::min(5)  // patience = 5 epochs
        .with_min_delta(0.001)
        .with_restore_best_weights(true);

    let mut early_stopping = EarlyStopping::new(early_stopping_config);

    println!("æ—©åœé…ç½®:");
    println!("  è€å¿ƒå€¼: 5 epochs");
    println!("  æœ€å°æ”¹å–„é˜ˆå€¼: 0.001");
    println!("  æ¢å¤æœ€ä½³æƒé‡: true\n");

    // ============================================================================
    // 6. è®­ç»ƒå¾ªç¯
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("6. å¼€å§‹è®­ç»ƒ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut global_step = 0;
    let mut best_val_loss = f32::INFINITY;
    let loss_fn = CrossEntropyLoss::new();

    for epoch in 0..epochs {
        let epoch_start = Instant::now();

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // è®­ç»ƒé˜¶æ®µ
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        let mut train_loss = 0.0_f32;
        let mut train_samples = 0.0_f32;

        for batch in train_loader.iter() {
            // æ›´æ–°å­¦ä¹ ç‡
            let current_lr = lr_scheduler.get_lr(global_step);
            update_optimizer_lr(&mut optimizer, current_lr);

            // è®­ç»ƒä¸€ä¸ª batch
            let batch_loss = train_batch(&mut model, &batch, &mut optimizer, &loss_fn, current_lr, n_classes);

            train_loss += batch_loss * batch.len() as f32;
            train_samples += batch.len() as f32;

            global_step += 1;

            // æå‰åœæ­¢ï¼ˆè¾¾åˆ°æ€»æ­¥æ•°ï¼‰
            if global_step >= total_steps {
                break;
            }
        }

        let avg_train_loss = train_loss / train_samples;

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // éªŒè¯é˜¶æ®µ
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        let mut val_loss = 0.0_f32;
        let mut val_samples = 0.0_f32;
        let mut correct = 0;

        for batch in val_loader.iter() {
            let (batch_loss, batch_correct) = validate_batch(&mut model, &batch, &loss_fn);

            val_loss += batch_loss * batch.len() as f32;
            val_samples += batch.len() as f32;
            correct += batch_correct;
        }

        let avg_val_loss = val_loss / val_samples;
        let val_accuracy = correct as f32 / val_samples as f32;

        let epoch_duration = epoch_start.elapsed();

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // æ‰“å°è®­ç»ƒä¿¡æ¯
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        println!(
            "Epoch {:3}/{:3} | Train Loss: {:.4} | Val Loss: {:.4} | Val Acc: {:.2}% | LR: {:.2e} | Time: {:.1}s",
            epoch + 1,
            epochs,
            avg_train_loss,
            avg_val_loss,
            val_accuracy * 100.0,
            lr_scheduler.get_lr(global_step.min(total_steps)),
            epoch_duration.as_secs_f32()
        );

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // ä¿å­˜æœ€ä½³æ¨¡å‹
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if avg_val_loss < best_val_loss {
            best_val_loss = avg_val_loss;
            // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            // checkpoint_manager.save(&model, epoch);
        }

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // æ—©åœæ£€æŸ¥
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if early_stopping.update(avg_val_loss, epoch) {
            println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            println!("âš  æ—©åœè§¦å‘ï¼");
            println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            early_stopping.print_summary();
            break;
        }

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // æå‰åœæ­¢ï¼ˆè¾¾åˆ°æ€»æ­¥æ•°ï¼‰
        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if global_step >= total_steps {
            println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            println!("âœ“ è¾¾åˆ°æ€»è®­ç»ƒæ­¥æ•° {}ï¼Œè®­ç»ƒå®Œæˆ", total_steps);
            println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");
            break;
        }
    }

    // ============================================================================
    // 7. è®­ç»ƒæ€»ç»“
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("7. è®­ç»ƒæ€»ç»“");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("è®­ç»ƒç»Ÿè®¡:");
    println!("  æ€»è®­ç»ƒæ­¥æ•°: {}", global_step);
    println!("  æœ€ä½³éªŒè¯æŸå¤±: {:.6}", best_val_loss);
    println!("  æœ€ä½³ epoch: {}", early_stopping.best_epoch());

    if early_stopping.should_stop() {
        println!("  æ—©åœè§¦å‘: Yes");
        println!("  åœæ­¢ epoch: {}", early_stopping.stopped_epoch().unwrap());
    } else {
        println!("  æ—©åœè§¦å‘: No");
        println!("  å®Œæˆæ‰€æœ‰ {} epochs", epochs);
    }

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     è®­ç»ƒå®Œæˆï¼                               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 8. å­¦ä¹ ç‡è°ƒåº¦å¯è§†åŒ–ï¼ˆæ–‡æœ¬ï¼‰
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("8. å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    visualize_lr_schedule(&lr_scheduler, total_steps);
}

/// è®­ç»ƒä¸€ä¸ª batch
fn train_batch(
    model: &mut TrainableTransformer,
    batch: &[(Vec<usize>, usize)],
    optimizer: &mut Adam,
    loss_fn: &CrossEntropyLoss,
    lr: f32,
    n_classes: usize,
) -> f32 {
    let mut total_loss = 0.0;

    for (input, target) in batch {
        // å°† Vec è½¬æ¢ä¸º Array2
        let input_array = vec_to_array2(input);

        // å°† target è½¬æ¢ä¸º one-hot ç¼–ç 
        let target_array = vec_to_onehot(*target, n_classes);

        // è®­ç»ƒä¸€æ­¥
        let (loss, _) = model.train_step(&input_array, &target_array, lr);
        total_loss += loss;
    }

    total_loss / batch.len() as f32
}

/// éªŒè¯ä¸€ä¸ª batch
fn validate_batch(
    model: &mut TrainableTransformer,
    batch: &[(Vec<usize>, usize)],
    loss_fn: &CrossEntropyLoss,
) -> (f32, usize) {
    let mut total_loss = 0.0;
    let mut correct = 0;

    for (input, target) in batch {
        // å°† Vec è½¬æ¢ä¸º Array2
        let input_array = vec_to_array2(input);

        // å‰å‘ä¼ æ’­
        let logits = model.forward(&input_array);

        // å°† target è½¬æ¢ä¸º one-hot ç¼–ç 
        let n_classes = logits.ncols();
        let target_array = vec_to_onehot(*target, n_classes);

        // è®¡ç®—æŸå¤±
        let loss = loss_fn.compute(&logits, &target_array);
        total_loss += loss;

        // è®¡ç®—å‡†ç¡®ç‡
        let predicted = argmax(&logits.row(0).to_vec());
        if predicted == *target {
            correct += 1;
        }
    }

    (
        total_loss / batch.len() as f32,
        correct,
    )
}

/// å°† target è½¬æ¢ä¸º one-hot ç¼–ç 
fn vec_to_onehot(target: usize, n_classes: usize) -> Array2<f32> {
    let mut onehot = vec![0.0f32; n_classes];
    onehot[target] = 1.0;
    Array2::from_shape_vec((1, n_classes), onehot).unwrap()
}

/// å°† Vec<usize> è½¬æ¢ä¸º Array2<usize>
fn vec_to_array2(vec: &[usize]) -> Array2<usize> {
    Array2::from_shape_vec((1, vec.len()), vec.to_vec()).unwrap()
}

/// Argmax
fn argmax(values: &[f32]) -> usize {
    let mut max_idx = 0;
    let mut max_val = values[0];

    for (i, &val) in values.iter().enumerate() {
        if val > max_val {
            max_val = val;
            max_idx = i;
        }
    }

    max_idx
}

/// æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡ï¼ˆé€šè¿‡åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼‰
fn update_optimizer_lr(optimizer: &mut Adam, new_lr: f32) {
    // ç”±äº Adam çš„å­¦ä¹ ç‡æ˜¯ç§æœ‰çš„ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ä¼˜åŒ–å™¨
    // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥æš´éœ² set_lr æ–¹æ³•
    let _ = new_lr; // å ä½ç¬¦ï¼Œå®é™…ä½¿ç”¨ä¸­éœ€è¦ä¼˜åŒ–å™¨æ”¯æŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
}

/// å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ–‡æœ¬å½¢å¼ï¼‰
fn visualize_lr_schedule(scheduler: &WarmupCosineAnnealing, total_steps: usize) {
    let num_points = 20;
    let step_size = total_steps / num_points;

    println!("æ­¥æ•°    | å­¦ä¹ ç‡      | é˜¶æ®µ");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    for i in 0..=num_points {
        let step = (i * step_size).min(total_steps);
        let lr = scheduler.get_lr(step);
        let phase = match scheduler.get_phase(step) {
            mini_transformer::WarmupPhase::Warmup => "Warmup  ",
            mini_transformer::WarmupPhase::Annealing => "Annealing",
            mini_transformer::WarmupPhase::Finished => "Finished",
        };

        println!("{:7} | {:.4e} | {}", step, lr, phase);
    }

    println!();

    println!("ğŸ’¡ å®è·µå»ºè®®:");
    println!("  - Warmup é˜¶æ®µæœ‰åŠ©äºç¨³å®šè®­ç»ƒåˆæœŸ");
    println!("  - ä½™å¼¦é€€ç«æœ‰åŠ©äºæ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜");
    println!("  - æ—©åœå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒèŠ‚çœè®­ç»ƒæ—¶é—´");
    println!("  - æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´è¶…å‚æ•°\n");
}
