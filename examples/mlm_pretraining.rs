//! Masked Language Modeling (MLM) é¢„è®­ç»ƒç¤ºä¾‹
//!
//! æ¼”ç¤º BERT é£æ ¼çš„é¢„è®­ç»ƒï¼šéšæœºæ©ç  tokens å¹¶é¢„æµ‹å®ƒä»¬

use mini_transformer::{
    MLMPretrainer, MLMConfig, apply_mlm_mask_batch, Vocabulary,
};
use ndarray::Array2;

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘   MLM é¢„è®­ç»ƒç¤ºä¾‹ (BERT é£æ ¼)                  â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 1. å‡†å¤‡é¢„è®­ç»ƒæ•°æ®
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("1. å‡†å¤‡é¢„è®­ç»ƒæ•°æ®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // æ¨¡æ‹Ÿé¢„è®­ç»ƒæ–‡æœ¬ï¼ˆå®é™…åº”è¯¥æ˜¯å¤§è§„æ¨¡æ–‡æœ¬ï¼‰
    let corpus = vec![
        "the cat sat on the mat",
        "dogs are loyal animals",
        "birds can fly high in the sky",
        "fish swim in the water",
        "trees grow tall and strong",
        "the sun is very bright today",
        "moon shines at night time",
        "stars twinkle in darkness",
        "children love to play games",
        "music brings joy to people",
        "reading books is very educational",
        "writing helps express thoughts",
        "learning new skills is important",
        "practice makes perfect results",
        "teamwork achieves great goals",
        "honesty is the best policy",
        "kindness matters to everyone",
        "courage overcomes many fears",
        "wisdom comes from experience",
        "patience leads to success",
    ];

    // æ„å»ºè¯æ±‡è¡¨
    let mut vocab = Vocabulary::new();
    for text in &corpus {
        for token in text.split_whitespace() {
            if !vocab.token_to_id.contains_key(&token.to_string()) {
                let id = vocab.vocab_size;
                vocab.token_to_id.insert(token.to_string(), id);
                vocab.id_to_token.push(token.to_string());
                vocab.vocab_size += 1;
            }
        }
    }

    // æ·»åŠ ç‰¹æ®Š tokens
    let mask_token_id = vocab.vocab_size;
    vocab.token_to_id.insert("<MASK>".to_string(), mask_token_id);
    vocab.id_to_token.push("<MASK>".to_string());
    vocab.vocab_size += 1;

    println!("è¯æ±‡è¡¨å¤§å°: {}", vocab.vocab_size);
    println!("ç‰¹æ®Š tokens: <UNK>=0, <PAD>=1, <MASK>={}\n", mask_token_id);

    // ç¼–ç è¯­æ–™
    let mut encoded_corpus = Vec::new();
    for text in &corpus {
        let tokens: Vec<usize> = text.split_whitespace()
            .map(|token| *vocab.token_to_id.get(token).unwrap_or(&0))
            .collect();
        encoded_corpus.push(tokens);
    }

    println!("ç¼–ç åçš„è¯­æ–™ç¤ºä¾‹:");
    println!("  åŸå§‹: {}", corpus[0]);
    println!("  ç¼–ç : {:?}\n", encoded_corpus[0]);

    // å¡«å……åˆ°å›ºå®šé•¿åº¦
    let max_seq_len = 10;
    for tokens in &mut encoded_corpus {
        while tokens.len() < max_seq_len {
            tokens.push(1); // PAD
        }
        if tokens.len() > max_seq_len {
            tokens.truncate(max_seq_len);
        }
    }

    // åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    let n_train = (encoded_corpus.len() * 8 / 10) as usize;

    let train_data: Vec<_> = encoded_corpus[..n_train].to_vec();
    let val_data: Vec<_> = encoded_corpus[n_train..].to_vec();

    // è½¬æ¢ä¸º Array2
    let mut train_flat = Vec::new();
    for tokens in &train_data {
        train_flat.extend(tokens);
    }
    let train_inputs = Array2::from_shape_vec((train_data.len(), max_seq_len), train_flat).unwrap();

    let mut val_flat = Vec::new();
    for tokens in &val_data {
        val_flat.extend(tokens);
    }
    let val_inputs = Array2::from_shape_vec((val_data.len(), max_seq_len), val_flat).unwrap();

    println!("è®­ç»ƒæ ·æœ¬: {}", train_data.len());
    println!("éªŒè¯æ ·æœ¬: {}\n", val_data.len());

    // ============================================================================
    // 2. æ¼”ç¤º MLM æ©ç ç­–ç•¥
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("2. MLM æ©ç ç­–ç•¥æ¼”ç¤º");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mlm_config = MLMConfig {
        mask_ratio: 0.3, // æé«˜åˆ° 30% ç”¨äºæ¼”ç¤º
        mask_prob: 0.8,
        random_prob: 0.1,
        keep_prob: 0.1,
        mask_token_id,
        pad_token_id: 1,
    };

    // å¯¹å‰å‡ ä¸ªæ ·æœ¬åº”ç”¨æ©ç 
    let demo_batch = train_inputs.slice(ndarray::s![0..3, ..]).to_owned();
    let (masked_demo, labels_demo) = apply_mlm_mask_batch(&demo_batch, &mlm_config, vocab.vocab_size);

    println!("BERT é£æ ¼æ©ç ç­–ç•¥ (15% tokens):\n");
    println!("  80% â†’ æ›¿æ¢ä¸º  æ ‡è®°");
    println!("  10% â†’ æ›¿æ¢ä¸ºéšæœº token");
    println!("  10% â†’ ä¿æŒä¸å˜\n");

    for i in 0..3 {
        println!("æ ·æœ¬ {}:", i + 1);
        println!("  åŸå§‹:  {}", decode_tokens(&train_inputs.row(i).to_vec(), &vocab));
        println!("  æ©ç :  {}", decode_tokens(&masked_demo.row(i).to_vec(), &vocab));
        println!("  æ ‡ç­¾:  {}", decode_tokens(&labels_demo.row(i).to_vec(), &vocab));
        println!();
    }

    // ============================================================================
    // 3. åˆ›å»º MLM é¢„è®­ç»ƒæ¨¡å‹
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("3. åˆ›å»º MLM é¢„è®­ç»ƒæ¨¡å‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut model = MLMPretrainer::new(
        vocab.vocab_size,  // vocab_size
        128,               // d_model
        4,                 // n_heads
        2,                 // n_layers
        256,               // d_ff
        max_seq_len,       // max_seq_len
    );

    println!("æ¨¡å‹æ¶æ„:");
    println!("  è¯æ±‡è¡¨å¤§å°: {}", vocab.vocab_size);
    println!("  æ¨¡å‹ç»´åº¦: {}", 128);
    println!("  æ³¨æ„åŠ›å¤´æ•°: {}", 4);
    println!("  Encoder å±‚æ•°: {}", 2);
    println!("  FFN ç»´åº¦: {}", 256);
    println!("  å‚æ•°æ€»æ•°: {}\n", model.param_count());

    // ============================================================================
    // 4. å‡†å¤‡é¢„è®­ç»ƒæ•°æ®ï¼ˆåº”ç”¨æ©ç ï¼‰
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("4. åº”ç”¨ MLM æ©ç åˆ°è®­ç»ƒæ•°æ®");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let (train_masked, train_labels) = apply_mlm_mask_batch(&train_inputs, &MLMConfig::default(), vocab.vocab_size);
    let (val_masked, val_labels) = apply_mlm_mask_batch(&val_inputs, &MLMConfig::default(), vocab.vocab_size);

    println!("è®­ç»ƒæ•°æ®: {} æ ·æœ¬", train_masked.nrows());
    println!("éªŒè¯æ•°æ®: {} æ ·æœ¬\n", val_masked.nrows());

    // ============================================================================
    // 5. å¼€å§‹é¢„è®­ç»ƒ
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("5. å¼€å§‹ MLM é¢„è®­ç»ƒ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let _result = model.train(
        &train_masked,
        &train_labels,
        &val_masked,
        &val_labels,
        50,      // epochs
        4,       // batch_size
        0.001,   // learning_rate
        10,      // patience
    );

    // ============================================================================
    // 6. æµ‹è¯•æ¨¡å‹
    // ============================================================================
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("6. æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    model.set_training(false);

    // åˆ›å»ºæµ‹è¯•æ ·æœ¬
    let test_sentences = vec![
        "the cat sat on the <MASK>",
        "dogs are <MASK> animals",
        "birds can <MASK> high in the sky",
    ];

    println!("å¡«ç©ºæµ‹è¯•:\n");

    for sentence in test_sentences {
        println!("  è¾“å…¥: {}", sentence);

        // ç¼–ç å¹¶å¡«å……
        let tokens: Vec<usize> = sentence.split_whitespace()
            .map(|token| {
                if token == "<MASK>" {
                    mask_token_id
                } else {
                    *vocab.token_to_id.get(token).unwrap_or(&0)
                }
            })
            .collect();

        let mut padded_tokens = tokens.clone();
        while padded_tokens.len() < max_seq_len {
            padded_tokens.push(1); // PAD
        }

        // å‰å‘ä¼ æ’­
        let input_array = Array2::from_shape_vec((1, max_seq_len), padded_tokens).unwrap();
        let logits = model.forward(&input_array);

        // æ‰¾åˆ°  ä½ç½®çš„é¢„æµ‹
        let mask_pos = tokens.iter().position(|&t| t == mask_token_id).unwrap();
        let mask_logits = logits.row(mask_pos);

        let mut top_candidates: Vec<(usize, f32)> = mask_logits.iter()
            .enumerate()
            .map(|(i, &logit)| (i, logit))
            .filter(|(i, _)| *i > 2) // æ’é™¤ç‰¹æ®Š tokens
            .collect();
        top_candidates.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        let top_5: Vec<(usize, f32)> = top_candidates.into_iter().take(5).collect();

        println!("  Top-5 é¢„æµ‹:");
        for (rank, (token_id, score)) in top_5.iter().enumerate() {
            let token = &vocab.id_to_token[*token_id];
            println!("    {}. '{}' (logit={:.3})", rank + 1, token, score);
        }
        println!();
    }

    // ============================================================================
    // 7. æŠ€æœ¯ç»†èŠ‚
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("MLM é¢„è®­ç»ƒæŠ€æœ¯ç»†èŠ‚");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("1. æ©ç ç­–ç•¥:");
    println!("   - éšæœºé€‰æ‹© 15% çš„ token ä½ç½®");
    println!("   - 80% æ¦‚ç‡æ›¿æ¢ä¸º  ");
    println!("   - 10% æ¦‚ç‡æ›¿æ¢ä¸ºéšæœº token");
    println!("   - 10% æ¦‚ç‡ä¿æŒåŸ token");
    println!();

    println!("2. æŸå¤±å‡½æ•°:");
    println!("   - åªåœ¨è¢«æ©ç çš„ä½ç½®è®¡ç®—æŸå¤±");
    println!("   - ä½¿ç”¨ Cross-Entropy Loss");
    println!("   - æ’é™¤  tokens");
    println!();

    println!("3. æ¨¡å‹æ¶æ„:");
    println!("   - åŸºäº Transformer Encoder");
    println!("   - åŒå‘æ³¨æ„åŠ›æœºåˆ¶");
    println!("   - MLM å¤´æ˜ å°„åˆ°è¯æ±‡è¡¨");
    println!();

    println!("4. é¢„è®­ç»ƒä¼˜åŠ¿:");
    println!("   - å­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡è¡¨ç¤º");
    println!("   - é€‚åˆç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NER ç­‰ï¼‰");
    println!("   - ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è‰¯å¥½çš„åˆå§‹åŒ–");
    println!();

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     MLM é¢„è®­ç»ƒç¤ºä¾‹å®Œæˆï¼                       â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("ğŸ’¡ æç¤º:");
    println!("   - å½“å‰ä½¿ç”¨å°è§„æ¨¡æ•°æ®æ¼”ç¤º");
    println!("   - å®é™…é¢„è®­ç»ƒéœ€è¦æ•°Båˆ°æ•°åB tokens");
    println!("   - é¢„è®­ç»ƒåå¯è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ");
}

/// è§£ç  tokens ä¸ºæ–‡æœ¬
fn decode_tokens(tokens: &[usize], vocab: &Vocabulary) -> String {
    tokens.iter()
        .filter_map(|&id| {
            if id < vocab.id_to_token.len() {
                Some(vocab.id_to_token[id].clone())
            } else {
                None
            }
        })
        .collect::<Vec<_>>()
        .join(" ")
}
