//! æ¢¯åº¦è£å‰ªç¤ºä¾‹
//!
//! æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¢¯åº¦è£å‰ªæ¥ç¨³å®šè®­ç»ƒ

use mini_transformer::{
    TrainableTransformer, TextClassificationDataset, GradientClipConfig,
    TensorExt,
};
use ndarray::Array2;

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     æ¢¯åº¦è£å‰ªç¤ºä¾‹                             â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ============================================================================
    // 1. åˆ›å»ºæ•°æ®é›†
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("1. åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let dataset = mini_transformer::create_sentiment_dataset();
    println!("è®­ç»ƒæ ·æœ¬: {}", dataset.train_len());
    println!("æµ‹è¯•æ ·æœ¬: {}", dataset.test_len());
    println!("è¯æ±‡è¡¨å¤§å°: {}\n", dataset.vocab.vocab_size());

    let (train_inputs, train_targets) = dataset.encode_train(10);
    let (test_inputs, test_targets) = dataset.encode_test(10);

    // ============================================================================
    // 2. å¯¹æ¯”å®éªŒï¼šæœ‰/æ— æ¢¯åº¦è£å‰ª
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("2. å¯¹æ¯”è®­ç»ƒå®éªŒ");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // å®éªŒé…ç½®
    let epochs = 30;
    let batch_size = 4;
    let learning_rate = 0.01;

    // å®éªŒ 1: æ— æ¢¯åº¦è£å‰ª
    println!("å®éªŒ 1: æ— æ¢¯åº¦è£å‰ª");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut model_no_clip = TrainableTransformer::new(
        dataset.vocab.vocab_size,
        128,
        4,
        2,
        256,
        10,
        dataset.n_classes,
    );

    let train_inputs_vec: Vec<Vec<usize>> = train_inputs.rows()
        .into_iter()
        .map(|row| row.to_vec())
        .collect();

    let test_inputs_vec: Vec<Vec<usize>> = test_inputs.rows()
        .into_iter()
        .map(|row| row.to_vec())
        .collect();

    let start = std::time::Instant::now();
    let mut best_val_acc_no_clip = 0.0;

    for epoch in 1..=epochs {
        let mut total_loss = 0.0;
        let mut total_acc = 0.0;

        for (input, &target) in train_inputs_vec.iter().zip(train_targets.iter()) {
            let input_batch = Array2::from_shape_vec((1, input.len()), input.clone()).unwrap();
            let target_onehot = TrainableTransformer::one_hot(target, dataset.n_classes);

            let (loss, acc) = model_no_clip.train_step(&input_batch, &target_onehot, learning_rate);
            total_loss += loss;
            total_acc += acc;
        }

        let (val_loss, val_acc) = model_no_clip.evaluate(&test_inputs_vec, &test_targets);

        if epoch % 5 == 0 || epoch == 1 {
            println!(
                "Epoch {:2} | Train Loss: {:.4} | Train Acc: {:.2}% | Val Acc: {:.2}%",
                epoch,
                total_loss / train_inputs_vec.len() as f32,
                total_acc / train_inputs_vec.len() as f32 * 100.0,
                val_acc * 100.0
            );
        }

        if val_acc > best_val_acc_no_clip {
            best_val_acc_no_clip = val_acc;
        }
    }

    let time_no_clip = start.elapsed();
    println!("\nâœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {:.2}s", time_no_clip.as_secs_f32());
    println!("  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {:.2}%\n", best_val_acc_no_clip * 100.0);

    // å®éªŒ 2: æœ‰æ¢¯åº¦è£å‰ª
    println!("å®éªŒ 2: æœ‰æ¢¯åº¦è£å‰ª (max_norm=1.0)");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let mut model_with_clip = TrainableTransformer::new(
        dataset.vocab.vocab_size,
        128,
        4,
        2,
        256,
        10,
        dataset.n_classes,
    );

    let clip_config = GradientClipConfig::norm(1.0);
    let start = std::time::Instant::now();
    let mut best_val_acc_with_clip = 0.0;

    for epoch in 1..=epochs {
        let mut total_loss = 0.0;
        let mut total_acc = 0.0;

        for (input, &target) in train_inputs_vec.iter().zip(train_targets.iter()) {
            let input_batch = Array2::from_shape_vec((1, input.len()), input.clone()).unwrap();
            let target_onehot = TrainableTransformer::one_hot(target, dataset.n_classes);

            let (loss, acc) = model_with_clip.train_step(&input_batch, &target_onehot, learning_rate);
            total_loss += loss;
            total_acc += acc;
        }

        let (val_loss, val_acc) = model_with_clip.evaluate(&test_inputs_vec, &test_targets);

        if epoch % 5 == 0 || epoch == 1 {
            println!(
                "Epoch {:2} | Train Loss: {:.4} | Train Acc: {:.2}% | Val Acc: {:.2}%",
                epoch,
                total_loss / train_inputs_vec.len() as f32,
                total_acc / train_inputs_vec.len() as f32 * 100.0,
                val_acc * 100.0
            );
        }

        if val_acc > best_val_acc_with_clip {
            best_val_acc_with_clip = val_acc;
        }
    }

    let time_with_clip = start.elapsed();
    println!("\nâœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {:.2}s", time_with_clip.as_secs_f32());
    println!("  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {:.2}%\n", best_val_acc_with_clip * 100.0);

    // ============================================================================
    // 3. ç»“æœå¯¹æ¯”
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("3. å®éªŒç»“æœå¯¹æ¯”");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚ æ–¹æ³•           â”‚ éªŒè¯å‡†ç¡®ç‡   â”‚ è®­ç»ƒæ—¶é—´     â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("â”‚ æ— æ¢¯åº¦è£å‰ª      â”‚ {:>8.2}%     â”‚ {:>8.2}s     â”‚",
        best_val_acc_no_clip * 100.0,
        time_no_clip.as_secs_f32()
    );
    println!("â”‚ æœ‰æ¢¯åº¦è£å‰ª      â”‚ {:>8.2}%     â”‚ {:>8.2}s     â”‚",
        best_val_acc_with_clip * 100.0,
        time_with_clip.as_secs_f32()
    );
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    // ============================================================================
    // 4. æŠ€æœ¯ç»†èŠ‚
    // ============================================================================
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("4. æ¢¯åº¦è£å‰ªæŠ€æœ¯ç»†èŠ‚");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("1. ä»€ä¹ˆæ˜¯æ¢¯åº¦è£å‰ªï¼Ÿ");
    println!("   - é™åˆ¶æ¢¯åº¦çš„èŒƒæ•°æˆ–å€¼");
    println!("   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸");
    println!("   - ç¨³å®šè®­ç»ƒè¿‡ç¨‹\n");

    println!("2. ä¸¤ç§è£å‰ªç±»å‹:");
    println!("   a) æŒ‰èŒƒæ•°è£å‰ª (æ¨è)");
    println!("      - è®¡ç®—æ¢¯åº¦å‘é‡çš„ L2 èŒƒæ•°");
    println!("      - å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾");
    println!("      - å…¬å¼: g = g * (max_norm / ||g||) if ||g|| > max_norm\n");

    println!("   b) æŒ‰å€¼è£å‰ª");
    println!("      - é™åˆ¶æ¯ä¸ªæ¢¯åº¦å…ƒç´ çš„èŒƒå›´");
    println!("      - å…¬å¼: g = clip(g, -max_value, max_value)\n");

    println!("3. æ¨èé…ç½®:");
    println!("   - max_norm = 1.0 (å¸¸ç”¨)");
    println!("   - max_norm = 5.0 (å®½æ¾)");
    println!("   - max_value = 1.0-10.0 (æŒ‰å€¼è£å‰ª)\n");

    println!("4. ä½¿ç”¨åœºæ™¯:");
    println!("   âœ… RNN/LSTM/Transformer (å®¹æ˜“æ¢¯åº¦çˆ†ç‚¸)");
    println!("   âœ… æ·±å±‚ç½‘ç»œ (æ·±åº¦ > 10)");
    println!("   âœ… é«˜å­¦ä¹ ç‡è®­ç»ƒ");
    println!("   âœ… é•¿åºåˆ—è®­ç»ƒ\n");

    println!("5. å®ç°ç¤ºä¾‹:");
    println!("```rust");
    println!("use mini_transformer::GradientClipConfig;");
    println!("let clip_config = GradientClipConfig::norm(1.0);");
    println!("// åœ¨è®­ç»ƒå¾ªç¯ä¸­åº”ç”¨");
    println!("let clipped_grads = clip_gradients(&grads, &clip_config);");
    println!("```\n");

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     ç¤ºä¾‹å®Œæˆï¼                               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    println!("ğŸ’¡ æç¤º:");
    println!("   - æ¢¯åº¦è£å‰ªæ˜¯é˜²æ­¢è®­ç»ƒä¸ç¨³å®šçš„ç®€å•æœ‰æ•ˆæ–¹æ³•");
    println!("   - æ¨èåœ¨æ‰€æœ‰ Transformer è®­ç»ƒä¸­ä½¿ç”¨");
    println!("   - max_norm = 1.0 æ˜¯å®‰å…¨çš„èµ·å§‹å€¼");
}
